[core]
# The home folder for airflow, default is ~/airflow
airflow_home = C:\Users\Akilesh\OneDrive\Desktop\Project_Pro\Big_Data\DBT\P3_DBT_Snowflake_Airflow_Netflix_Project\airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = C:\Users\Akilesh\OneDrive\Desktop\Project_Pro\Big_Data\DBT\P3_DBT_Snowflake_Airflow_Netflix_Project\airflow\dags

# The folder where airflow should store its log files
base_log_folder = C:\Users\Akilesh\OneDrive\Desktop\Project_Pro\Big_Data\DBT\P3_DBT_Snowflake_Airflow_Netflix_Project\airflow\logs

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = SequentialExecutor

# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost/airflow_db

# The encoding for the databases
sql_engine_encoding = utf-8

# How long before timing out a python file import while filling the DagBag
dagbag_import_timeout = 30.0

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# The local timezone
default_timezone = utc
